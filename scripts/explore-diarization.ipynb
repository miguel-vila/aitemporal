{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2b06e5-03dc-4113-adaf-f23c75e8e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments file for model medium not found, skipping...\n",
      "Segments file for model large not found, skipping...\n",
      "Segments file for model turbo not found, skipping...\n",
      "Model: tiny\n",
      " Por la nocoda, y muy interesante,  André de ese en este momento de la vida de Colombia,  que son las lecciones que se derivan,  de esa época,  una lección es la oposición por la humidad de la que conversa en el terreno.  No puedo pasar en Colombia una dispersión de la oposición  que deje el poder totalmente en manos del presidente.  Carlos Coyero ha sido un ministro director de Fesarrollo,  miembro de la Junta Erectiva del Banco de la República,  también tiene una maestrera en historia\n",
      "\n",
      "\n",
      "Model: base\n",
      " Para una cosa muy interesante, Andrés, en este momento de la vida colombiana, que son las lecciones que se derivan de esa época.  Una lección es la oposición colombiana de la, tienen que conversar entre ellos.  No puede pasar en Colombia una dispersión de la oposición que dejé el poder totalmente en manos del presidente.  Carlos Cajero ha sido un ministro, director de desarrollo, miembro de la Junta de la Dirección de la República.  También tiene una maestria en historia y es el fundador de la escuela de gobierno de la Universidad de los Antes.  Usted que ha estudiado muchas días, que ha dictado cursos sobre el vertilleras que amargo.  Ayúe me entendera cómo se porfó, cómo se formó este personaje.  Yo creo que ese fue una característica siempre al vertilleras.  Hay una relación espectacular entre Llega Sí y Kennedy.  Y hay una cosa muy interesante cuando le preguntan a ella que Kennedy después de el asesinato de Kennedy le preguntan, ¿Quién es el estadista que más le ha impresionado?\n",
      "\n",
      "\n",
      "Model: small\n",
      " Pero hay una cosa muy interesante a Andrés en este momento de la vida colombiana, que  son las lecciones que se derivan de esa época.  Una lección es la oposición colombiana, tiene que conversar entre sí.  No puede pasar en Colombia una dispersión de la oposición que dejé el poder totalmente  en manos del presidente.  Carlos Caballero ha sido ministro, director de Fedez Arroyo, miembro de la Junta Directiva  del Banco de la República.  También tiene una maestría en historia y es el fundador de la Escuela de Gobierno de  la Universidad de los Andes.  Usted que ha estudiado mucho ayer, que ha dictado cursos sobre Alberto Yeras Camargo,\n",
      "\n",
      "\n",
      "Processing audio file: ./audio/8-XQtwZPowY.wav. Running on device: mps\n",
      "Loading cached segments from ./audio/8-XQtwZPowY.wav-segments-tiny.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "/Users/miguelvilagonzalez/repos/aitemporal/scripts/.venv/lib/python3.13/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m video_id = \u001b[33m'\u001b[39m\u001b[33m8-XQtwZPowY\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m compare_models_segments(video_id)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtranscribe_and_diarize_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./audio/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvideo_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.wav\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/aitemporal/scripts/audio_processing.py:69\u001b[39m, in \u001b[36mtranscribe_and_diarize_audio\u001b[39m\u001b[34m(audio_path)\u001b[39m\n\u001b[32m     66\u001b[39m diarization_pipeline.to(device)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Perform diarization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m diarization = \u001b[43mdiarization_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDiarization completed for:\u001b[39m\u001b[33m\"\u001b[39m, audio_path)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Align transcription with diarization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/aitemporal/scripts/.venv/lib/python3.13/site-packages/pyannote/audio/core/pipeline.py:327\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, file, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpreprocessors\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    325\u001b[39m     file = ProtocolFile(file, lazy=\u001b[38;5;28mself\u001b[39m.preprocessors)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/aitemporal/scripts/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_diarization.py:523\u001b[39m, in \u001b[36mSpeakerDiarization.apply\u001b[39m\u001b[34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[39m\n\u001b[32m    520\u001b[39m     embeddings = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_exclude_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     hook(\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m, embeddings)\n\u001b[32m    530\u001b[39m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/aitemporal/scripts/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_diarization.py:348\u001b[39m, in \u001b[36mSpeakerDiarization.get_embeddings\u001b[39m\u001b[34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[39m\n\u001b[32m    345\u001b[39m mask_batch = torch.vstack(masks)\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m embedding_batch: np.ndarray = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwaveform_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_batch\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[32m    353\u001b[39m embedding_batches.append(embedding_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/aitemporal/scripts/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_verification.py:376\u001b[39m, in \u001b[36mSpeechBrainPretrainedSpeakerEmbedding.__call__\u001b[39m\u001b[34m(self, waveforms, masks)\u001b[39m\n\u001b[32m    370\u001b[39m wav_lens = wav_lens / max_len\n\u001b[32m    371\u001b[39m wav_lens[too_short] = \u001b[32m1.0\u001b[39m\n\u001b[32m    373\u001b[39m embeddings = (\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     .numpy()\n\u001b[32m    378\u001b[39m )\n\u001b[32m    380\u001b[39m embeddings[too_short.cpu().numpy()] = np.nan\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from audio_processing import transcribe_and_diarize_audio, compare_models_segments\n",
    "\n",
    "video_id = '8-XQtwZPowY'\n",
    "compare_models_segments(video_id)\n",
    "transcribe_and_diarize_audio(f'./audio/{video_id}.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
